#+LATEX_HEADER: \usepackage[margin=0.5in]{geometry}
#+OPTIONS: toc:nil

#+HUGO_BASE_DIR: /home/shane/var/smulliga/source/git/semiosis/semiosis-hugo
#+HUGO_SECTION: ./posts

#+TITLE: Beyond the boundaries of paradigms
#+DATE: <2021-09-07>
#+AUTHOR: Shane Mulligan
#+KEYWORDS: gpt blockchain language

** Examples of ordinary programming paradigms
- Type-Level programming
  - https://rebeccaskinner.net/posts/2021-08-25-introduction-to-type-level-programming.html
- Relational Programming
  - https://github.com/webyrd/dissertation-single-spaced/blob/master/thesis.pdf
- Functional programming
- Object-oriented programming
- Imperative programming

** Questions to ponder
- Where do ordinary programming paradigms end and what exists between and beyond them?
- Do there exist boundaries between physical, imaginary and metaphysical languages?
- What constitutes a new paradigm in programming?
  Arguably something that is orthogonal or unique to the rest.
- May Imaginary Programming (IP) be considered a new programming paradigm?
  - I think it can be.
- Is the past mutable?
  - I think it's possible. If one makes their reality a
    a simulation of the future then they may
    change the past.
  - But is it far enough to send a message 30+ years?
  - Metaphysical language?

** Hierarchy of languages
- Physical languages
  - Zo√∂semiosis
- Imaginary languages
  - World languages
    - English
    - Japanese
    - Sign language
  - Natural languages
    - Non-verbal body language
  - Formal languages
    - Markdown
    - C++
- Metaphysical languages
  - Intellectual without cognitive materials

** Thoughts on Imaginary Programming
The fundamental theorem of algebra states that
every non-constant single-variable polynomial
with complex coefficients has at least one
complex root.

The fundamental theorem of algebra implies that
solutions to polynomials are only available
when adding the imaginary dimension.

Likewise, solutions to problems such as
translating between world languages require
the addition of multiple dimensions.

Historically, words and phrases may be
modelled by a vector space of 200 dimensions,
i.e. FastText.

Nowadays we use tensors which are n-ranked
matrices that lazily yield their values (i.e.
sparse matrix optimisation, and TF optimises
out computation of the values in tensor
cells).

However, like the Bohr model of the atom, an
n-dimensional vector-space is a useful
concept.

Large LMs such as GPT-X are a representation
of a superset of languages, the languages of
the data they are trained on.

** Speculation of LMs of the future
They may serve as snapshots of a given time
period, and may be used to generatively
recreate experiences from the time they were
created.

These may be recorded on blockchain. Post-
human ocean marketplace perhaps.

For example, GPT-3 may one day be used to re-
create YouTube via simulation.

I think it's rather obvious that the world is
heading for building simulations of the
internet within LMs.

My worry is that corporations with too much
power, or nation states will leverage their
LMs to create information bubbles which are
increasingly difficult to escape from, and
traverse between.