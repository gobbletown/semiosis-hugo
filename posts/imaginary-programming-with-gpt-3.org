#+LATEX_HEADER: \usepackage[margin=0.5in]{geometry}
#+OPTIONS: toc:nil

#+HUGO_BASE_DIR: /home/shane/var/smulliga/source/git/semiosis/semiosis-hugo
#+HUGO_SECTION: ./posts

#+TITLE: Imaginary programming with GPT-3
#+DATE: <2021-04-08>
#+AUTHOR: Shane Mulligan
#+KEYWORDS: gpt openai

+ Code :: https://github.com/semiosis/pen.el

+ Prompts :: https://github.com/semiosis/prompts/

+ Disclaimer :: Please contribute as this is an open source project! It's very hard to find free prompts online currently and that's because everyone is out for themselves. Please support open source. Thank you.

+ Glossary :: http://github.com/semiosis/glossaries-gh/blob/master/imaginary-programming.txt

** Summary
This is a demonstration of an imaginary
programming environment. There may be nothing
else like it in the world today.

The world needs to get ready for the next
generations of Large LMs, such as =GPT-4=.

Indeed, they will be able to dream up the
entire experience of a textual user interface
by inferring how it changes based on keyboard
input. That is because a =tty= is simple text
stream.

A current generation GPT trained on =tty=
recordings would most certainly probably be
able to do this already, but with =GPT-4= and
future LMs comes convenience in the form of
prompt tuning.

An imaginary programming environment is [[https://github.com/semiosis/pen.el/blob/master/docs/README.org][one of the supported features]] of =pen.el=.

*** What does it mean to be imaginary?
Several of the components of a normal
programming environment are replaced by
functions that infer rather than evaluate.

These components are replaced by GPT-3 prompts.
- linter,
- indenter/reformatter,
- code completion,
- interpreter

Imaginary seems to be a good word because:
- GPT-3 is imagining the environment, the code and the output.
- the languages are kinda like imaginary numbers. They are all understood within the same language model, kinda like a coordinate space.
- It lets you be very imaginative!

*** The implications
- There are no interpreters or compilers being used but you can still generate code using words and evaluate code by inference.
- All requests take the same amount of time to run.
- All languages have equally facilitated environments.
- You may program in any language.
- It runs broken code.
- One day you may design new languages within the latent space of GPT-3, without doing any programming.
- You may have an interpreter for languages with no interpreter, such as C++.
- You may have code completion for scripting languages without a completion engine.
- You may use it for languages which are dead and an interpreter is not available.
- It's great for thought experiments.

*** Literate programming
Imaginary programming seems to be an orthogonal extension to literate programming.

According to Wikipedia, _literate programming_
is A programming paradigm introduced by Donald
Knuth in which a computer program is given an
explanation of its logic in a NL, such as
English, interspersed with snippets of macros
and traditional source code, from which
compilable source code can be generated.[1]
The approach is used in scientific computing
and in data science routinely for reproducible
research and open access purposes.[2] Literate
programming tools are used by millions of
programmers today.[3]

+ Literate programming :: https://en.wikipedia.org/wiki/Literate_programming

*** Current progress
Some of the plumbing, including the
interpreter and the autocompletion system are
in an =MVP= stage.

** Demonstration
#+BEGIN_EXPORT html
<!-- Play on asciinema.com -->
<!-- <a title="asciinema recording" href="https://asciinema.org/a/G8HPLtlCWTQIzGssLrM3ZvxhT" target="_blank"><img alt="asciinema recording" src="https://asciinema.org/a/G8HPLtlCWTQIzGssLrM3ZvxhT.svg" /></a> -->
<!-- Play on the blog -->
<script src="https://asciinema.org/a/G8HPLtlCWTQIzGssLrM3ZvxhT.js" id="asciicast-G8HPLtlCWTQIzGssLrM3ZvxhT" async></script>
#+END_EXPORT

** Glossary
#+BEGIN_SRC text -n :async :results verbatim code
  imaginary programming
  IP
      A programming paradigm in which a computer
      program's behaviour exists in relativity to
      language models.

      To make an analogy, imaginary programming
      is a type of programming, where much like
      pure-functional, code who's behaviour
      depends on the output of a language model,
      either pending or precomputed, stands
      apart from that which has no such
      association.

      What is deemed 'imaginary' is code has or
      will be be 'contaminated' by output from a
      language and behaviour so altered by it.

      What is deemed 'ordinary' is code which is
      not 'imaginary'.

      A distinction is made between grounded and
      non-grounded imaginary programming.

  grounded
  grounded imaginary programming
  non-grounded
  pure imaginary programming
      If an imaginary function has the language
      model as a parameter then it is considered
      'grounded'.

      If an imaginary function relies on the
      output of functions that have in the past
      used a language model as a parameter, but
      does not contain a reference to specific
      language model used then it is considered
      'non-grounded' or 'pure imaginary'.

      Non-grounded code is still code in a
      similar way to how pure functional code is
      considered code.

  ordinary programming
      Programming with functions that do not
      have an imaginary dimension (they do not
      take a language model as a parameter and
      the result of ordinary code is not
      polluted by a language model).

  holographic programming
      [type of imaginary programming]

      This is like imaginary programming but
      where the language models are trained on
      software.

      The code being written, therefore, is
      employing associations made between
      elements of the code, how that code is
      used and how it is described, to build
      applications.

      This lets you code within the hyperspace
      of a language model to enable things like:
      - reflection
      - inference in place of computation

  literate programming
      A programming paradigm that emphasizes the
      importance of the reading and writing of
      code over the importance of the design of
      the code.
#+END_SRC

Imaginary programming is a programming
paradigm in which a computer program's
behaviour exists in relativity to language models (LM), such as
OpenAI's GPT-3.

To make an analogy, imaginary programming is a
type of programming, where similarly to pure-
functional programming, the subset of code
who's behaviour depends on the output of a LM
(either pending or precomputed outputs) stands
apart from the rest of the code that has no
such association. What is deemed /imaginary/
code is code that uses or is waiting on output
from a LM and its behaviour is so altered by
it. What is deemed /ordinary/ code is code
which is not 'imaginary'.

A distinction is made between grounded and
non-grounded imaginary programming. /Non-grounded imaginary programming/ may also be
called /pure imaginary programming/. If an
imaginary function has the LM as a parameter
then it is considered /grounded/. If an
imaginary function relies on the output of
functions that have in the past used a LM as a
parameter, but does not contain a reference to
specific LM used then it is considered /non-grounded/ or /pure imaginary/. Non-grounded
code is still code in a similar way to how
pure functional code is considered code.

In contrast to imaginary programming, ordinary
programming refers to the style of programming
where functions have no dependence on the
output of a LM and thus have no imaginary
dimension.

/holographic programming/ is a type of
imaginary programming where the LMs being used
are trained on a dataset that is comprised of
software. The imaginary code being written,
therefore, may employ associations made
between elements of the code during the
training of the LM, and possibly how the
application is used, to build applications.
Such appliations may be based on knowledge of
how a program has been used in the past and
not contain any of the original source code.

** Prompts
*** kickstarter
This prompt initiates the imaginary interpreter / REPL.

#+BEGIN_SRC yaml -n :async :results verbatim code
  title: "Code interpreter kickstarter"
  future-titles: "Code interpreter kickstarter"
  # It may be combined with a prompt more adept at continuing conversation
  # aims: |+
  # - More abstractive rewording
  doc: "Given a line of code, infer the result of running that code"
  # aims: |+
  # - More abstractive rewording
  prompt-version: 1
  # <:pp> defines a point where the following
  # text is concatenated before the postprocessor
  # is run.
  # <1>, <2> etc. are where variables are substituted
  prompt: |+
      Code examples:
  
      # Language: Python
      print(random.randint(0,9))
      # Output: 5
      ###
      # Language: Bash
      Str="Learn Linux from LinuxHint"
      subStr=${Str:6:5}
      # Output: Linux
      ###
      # Language: <1>
      <2>
      # Output:
  problems:
  - "Struggles with the latter columns."
  # # Additional transformation of prompt after the template
  # prompt-filter: "sed -z 's/\s\+$//'"
  # # Trailing whitespace is always removed
  # prompt-remove-trailing-whitespace: on
  # myrc will select the completion engine using my config.
  # This may be openi-complete or something else
  engine: "myrc"
  # if nothing is selected in myrc and openapi-complete is used
  # by default, then openai should select this engine.
  preferred-openai-engine: "davinci"
  # 0.0 = /r/hadastroke
  # 1.0 = /r/iamveryrandom
  # Use 0.3-0.8
  temperature: 0.8
  max-tokens: 60
  top-p: 1.0
  # Not available yet: openai api completions.create --help
  frequency-penalty: 0.5
  # If I make presence-penalty 0 then it will get very terse
  presence-penalty: 0.0
  best-of: 1
  # Only the first one will be used by the API,
  # but the completer script will use the others.
  # Currently the API can only accept one stop-sequence, but that may change.
  stop-sequences:
  # - "\n"
  # - "\n\n"
  # 2 hashes is more reliable as a stop sequence because
  # sometimes the generation morphs from ### to ##
  - "##"
  inject-start-text: yes
  inject-restart-text: yes
  show-probabilities: off
  # Cache the function by default when running the prompt function
  cache: on
  vars:
  - "language"
  - "code"
  examples:
  - "boysenberries"
  - "strawberries"
  # Completion is for generating a company-mode completion function
  # completion: on
  # # default values for pen -- evaled
  # # This is useful for completion commands.
  # pen-defaults:
  # - "(detect-language)"
  # - "(pen-preceding-text)"
  # These are elisp String->String functions and run from pen.el
  # It probably runs earlier than the preprocessors shell scripts
  # pen-preprocessors:
  # - "pen-pf-correct-code"
  # # A preprocessor filters the var at that position
  # the current implementation of preprocessors is kinda slow and will add ~100ml per variable
  # # This may be useful to distinguish a block of text, for example
  # preprocessors:
  # - "sed 's/^/- /"
  # - "cat"
  chomp-start: on
  chomp-end: off
  prefer-external: on
  # This is an optional external command which may be used to perform the same task as the API.
  # This can be used to train the prompt.
  # The external command must take arguments, not stdin
  # So that all variables may be passed into it.
  external: "generate-text-from-input.sh"
  # This compares the output of the external script to the output of the LM
  similarity-test:
  # This script returns a 0-1 decimal value representing the quality of the generated output.
  # The input is 2 arguments each containing output
  # The output is a decimal number from 0 to 1
  quality-script: "my-quality-checker-for-this-prompt.sh"
  # This script can be used to validate the output.
  # If the output is accurate, the validation script returns exit code 1.
  # The input is 2 arguments each containing output
  validation-script: "my-validator-for-this-prompt.sh"
  # Enable running conversation
  conversation-mode: no
  # Replace selected text
  filter: no
  # Keep stitching together until reaching this limit
  # This allows a full response for answers which may need n*max-tokens to reach the stop-sequence.
  stitch-max: 0
  needs-work: no
  n-test-runs: 5
  # Prompt function aliases
  # aliases:
  # - "asktutor"
  # postprocessor: "sed 's/- //' | uniqnosort"
  # # Run it n times and combine the output
  # n-collate: 10
  # This for combining prompts:
  # It might be, for example, summarize, or uniqnosort
  # collation-postprocessor: "uniqnosort"
#+END_SRC

*** conjugator
This prompt is part of the _interpreter conjugator_,
and enables the _imaginary interpreter_ to continue the "conversation" more optimally than the _kickstarter_ prompt.

#+BEGIN_SRC yaml -n :async :results verbatim code
  title: "Code interpreter conjugator"
  future-titles:
  aims:
  doc: "Given some lines of code, coerce them into prompt format then continue"
  prompt-version: 1
  prompt: |+
      <1>
      Output:
  # prompt-filter: "sed -z 's/\s\+$//'"
  # # Trailing whitespace is always removed
  # prompt-remove-trailing-whitespace: on
  # myrc will select the completion engine using my config.
  # This may be openi-complete or something else
  engine: "myrc"
  # if nothing is selected in myrc and openapi-complete is used
  # by default, then openai should select this engine.
  preferred-openai-engine: "davinci"
  # 0.0 = /r/hadastroke
  # 1.0 = /r/iamveryrandom
  # Use 0.3-0.8
  temperature: 0.8
  max-tokens: 60
  top-p: 1.0
  # Not available yet: openai api completions.create --help
  frequency-penalty: 0.5
  # If I make presence-penalty 0 then it will get very terse
  presence-penalty: 0.0
  best-of: 1
  # Only the first one will be used by the API,
  # but the completer script will use the others.
  # Currently the API can only accept one stop-sequence, but that may change.
  stop-sequences:
  # - "\n"
  # - "\n\n"
  # 2 hashes is more reliable as a stop sequence because
  # sometimes the generation morphs from ### to ##
  - "##"
  inject-start-text: yes
  inject-restart-text: yes
  show-probabilities: off
  # Cache the function by default when running the prompt function
  cache: on
  vars:
  - "former"
  - "latter"
  examples:
  - "boysenberries"
  - "strawberries"
  # Completion is for generating a company-mode completion function
  # completion: on
  # # default values for pen -- evaled
  # # This is useful for completion commands.
  # pen-defaults:
  # - "(detect-language)"
  # - "(pen-preceding-text)"
  # These are elisp String->String functions and run from pen.el
  # It probably runs earlier than the preprocessors shell scripts
  pen-preprocessors:
  - "pen-pf-correct-grammar"
  # # A preprocessor filters the var at that position
  # the current implementation of preprocessors is kinda slow and will add ~100ml per variable
  # # This may be useful to distinguish a block of text, for example
  # preprocessors:
  # - "sed 's/^/- /"
  # - "cat"
  chomp-start: on
  chomp-end: off
  prefer-external: on
  # This is an optional external command which may be used to perform the same task as the API.
  # This can be used to train the prompt.
  # The external command must take arguments, not stdin
  # So that all variables may be passed into it.
  external: "generate-text-from-input.sh"
  # This compares the output of the external script to the output of the LM
  similarity-test:
  # This script returns a 0-1 decimal value representing the quality of the generated output.
  # The input is 2 arguments each containing output
  # The output is a decimal number from 0 to 1
  quality-script: "my-quality-checker-for-this-prompt.sh"
  # This script can be used to validate the output.
  # If the output is accurate, the validation script returns exit code 1.
  # The input is 2 arguments each containing output
  validation-script: "my-validator-for-this-prompt.sh"
  # Enable running conversation
  conversation-mode: yes
  # This is the name of an external database-driven pretext generator.
  # It would typically summarize and fact extract from history.
  # It then passes the pretext to the new prompt.
  conversation-pretext-generator: "code-interpreter"
  # Replace selected text
  filter: no
  # Keep stitching together until reaching this limit
  # This allows a full response for answers which may need n*max-tokens to reach the stop-sequence.
  stitch-max: 0
  needs-work: no
  n-test-runs: 5
  # Prompt function aliases
  # aliases:
  # - "asktutor"
  # postprocessor: "sed 's/- //' | uniqnosort"
  # # Run it n times and combine the output
  # n-collate: 10
  # This for combining prompts:
  # It might be, for example, summarize, or uniqnosort
  # collation-postprocessor: "uniqnosort"
#+END_SRC