#+LATEX_HEADER: \usepackage[margin=0.5in]{geometry}
#+OPTIONS: toc:nil

#+HUGO_BASE_DIR: /home/shane/var/smulliga/source/git/semiosis/semiosis-hugo
#+HUGO_SECTION: ./posts

#+TITLE: Imaginary programming with GPT-3
#+DATE: <2021-04-08>
#+AUTHOR: Shane Mulligan
#+KEYWORDS: GPT-3

+ Code :: https://github.com/semiosis/pen.el

+ Prompts :: https://github.com/semiosis/prompts/

---

+ Disclaimer :: Please contribute as this is an open source project! It's very hard to find free prompts online currently and that's because everyone is out for themselves. Please support open source. Thank you.

** Summary
This is a demonstration of an imaginary
programming environment. There may be nothing
else like it in the world today.

*** What does it mean to be imaginary?
Several of the components of a normal
programming environment are replaced by
functions that infer rather than evaluate.

These components are replaced by GPT-3 prompts.
- linter,
- indenter/reformatter,
- code completion,
- interpreter

*** The implications
- There are no interpreters or compilers being used but you can still generate code using words and evaluate code by inference.
- All requests take the same amount of time to run.
- All languages have equally facilitated environments.
- You may program in any language
- It's runs broken code
- One day you may design new languages within the latent space of GPT-3, without doing any programming.
- You may have an interpreter for languages with no interpreter, such as C++.
- You may have code completion for scripting languages without a completion engine.
- You may use it for languages which are dead and an interpreter is not available.
- It's great for thought experiments.

*** Current progress
Some of the plumbing, including the
interpreter and the autocompletion system are
in an =MVP= stage.

** Demonstration

#+BEGIN_EXPORT html
<!-- Play on asciinema.com -->
<!-- <a title="asciinema recording" href="https://asciinema.org/a/G8HPLtlCWTQIzGssLrM3ZvxhT" target="_blank"><img alt="asciinema recording" src="https://asciinema.org/a/G8HPLtlCWTQIzGssLrM3ZvxhT.svg" /></a> -->
<!-- Play on the blog -->
<script src="https://asciinema.org/a/G8HPLtlCWTQIzGssLrM3ZvxhT.js" id="asciicast-G8HPLtlCWTQIzGssLrM3ZvxhT" async></script>
#+END_EXPORT

** Prompts
*** kickstarter
This prompt initiates the imaginary interpreter / REPL.

#+BEGIN_SRC yaml -n :async :results verbatim code
  title: "Code interpreter kickstarter"
  future-titles: "Code interpreter kickstarter"
  # It may be combined with a prompt more adept at continuing conversation
  # aims: |+
  # - More abstractive rewording
  doc: "Given a line of code, infer the result of running that code"
  # aims: |+
  # - More abstractive rewording
  prompt-version: 1
  # <:pp> defines a point where the following
  # text is concatenated before the postprocessor
  # is run.
  # <1>, <2> etc. are where variables are substituted
  prompt: |+
      Code examples:
  
      # Language: Python
      print(random.randint(0,9))
      # Output: 5
      ###
      # Language: Bash
      Str="Learn Linux from LinuxHint"
      subStr=${Str:6:5}
      # Output: Linux
      ###
      # Language: <1>
      <2>
      # Output:
  problems:
  - "Struggles with the latter columns."
  # # Additional transformation of prompt after the template
  # prompt-filter: "sed -z 's/\s\+$//'"
  # # Trailing whitespace is always removed
  # prompt-remove-trailing-whitespace: on
  # myrc will select the completion engine using my config.
  # This may be openi-complete or something else
  engine: "myrc"
  # if nothing is selected in myrc and openapi-complete is used
  # by default, then openai should select this engine.
  preferred-openai-engine: "davinci"
  # 0.0 = /r/hadastroke
  # 1.0 = /r/iamveryrandom
  # Use 0.3-0.8
  temperature: 0.8
  max-tokens: 60
  top-p: 1.0
  # Not available yet: openai api completions.create --help
  frequency-penalty: 0.5
  # If I make presence-penalty 0 then it will get very terse
  presence-penalty: 0.0
  best-of: 1
  # Only the first one will be used by the API,
  # but the completer script will use the others.
  # Currently the API can only accept one stop-sequence, but that may change.
  stop-sequences:
  # - "\n"
  # - "\n\n"
  # 2 hashes is more reliable as a stop sequence because
  # sometimes the generation morphs from ### to ##
  - "##"
  inject-start-text: yes
  inject-restart-text: yes
  show-probabilities: off
  # Cache the function by default when running the prompt function
  cache: on
  vars:
  - "language"
  - "code"
  examples:
  - "boysenberries"
  - "strawberries"
  # Completion is for generating a company-mode completion function
  # completion: on
  # # default values for pen -- evaled
  # # This is useful for completion commands.
  # pen-defaults:
  # - "(detect-language)"
  # - "(pen-preceding-text)"
  # These are elisp String->String functions and run from pen.el
  # It probably runs earlier than the preprocessors shell scripts
  # pen-preprocessors:
  # - "pen-pf-correct-code"
  # # A preprocessor filters the var at that position
  # the current implementation of preprocessors is kinda slow and will add ~100ml per variable
  # # This may be useful to distinguish a block of text, for example
  # preprocessors:
  # - "sed 's/^/- /"
  # - "cat"
  chomp-start: on
  chomp-end: off
  prefer-external: on
  # This is an optional external command which may be used to perform the same task as the API.
  # This can be used to train the prompt.
  # The external command must take arguments, not stdin
  # So that all variables may be passed into it.
  external: "generate-text-from-input.sh"
  # This compares the output of the external script to the output of the LM
  similarity-test:
  # This script returns a 0-1 decimal value representing the quality of the generated output.
  # The input is 2 arguments each containing output
  # The output is a decimal number from 0 to 1
  quality-script: "my-quality-checker-for-this-prompt.sh"
  # This script can be used to validate the output.
  # If the output is accurate, the validation script returns exit code 1.
  # The input is 2 arguments each containing output
  validation-script: "my-validator-for-this-prompt.sh"
  # Enable running conversation
  conversation-mode: no
  # Replace selected text
  filter: no
  # Keep stitching together until reaching this limit
  # This allows a full response for answers which may need n*max-tokens to reach the stop-sequence.
  stitch-max: 0
  needs-work: no
  n-test-runs: 5
  # Prompt function aliases
  # aliases:
  # - "asktutor"
  # postprocessor: "sed 's/- //' | uniqnosort"
  # # Run it n times and combine the output
  # n-collate: 10
  # This for combining prompts:
  # It might be, for example, summarize, or uniqnosort
  # collation-postprocessor: "uniqnosort"
#+END_SRC

*** conjugator
This prompt is part of the _interpreter conjugator_,
and enables the _imaginary interpreter_ to continue the "conversation" more optimally than the _kickstarter_ prompt.

#+BEGIN_SRC yaml -n :async :results verbatim code
  title: "Code interpreter conjugator"
  future-titles:
  aims:
  doc: "Given some lines of code, coerce them into prompt format then continue"
  prompt-version: 1
  prompt: |+
      <1>
      Output:
  # prompt-filter: "sed -z 's/\s\+$//'"
  # # Trailing whitespace is always removed
  # prompt-remove-trailing-whitespace: on
  # myrc will select the completion engine using my config.
  # This may be openi-complete or something else
  engine: "myrc"
  # if nothing is selected in myrc and openapi-complete is used
  # by default, then openai should select this engine.
  preferred-openai-engine: "davinci"
  # 0.0 = /r/hadastroke
  # 1.0 = /r/iamveryrandom
  # Use 0.3-0.8
  temperature: 0.8
  max-tokens: 60
  top-p: 1.0
  # Not available yet: openai api completions.create --help
  frequency-penalty: 0.5
  # If I make presence-penalty 0 then it will get very terse
  presence-penalty: 0.0
  best-of: 1
  # Only the first one will be used by the API,
  # but the completer script will use the others.
  # Currently the API can only accept one stop-sequence, but that may change.
  stop-sequences:
  # - "\n"
  # - "\n\n"
  # 2 hashes is more reliable as a stop sequence because
  # sometimes the generation morphs from ### to ##
  - "##"
  inject-start-text: yes
  inject-restart-text: yes
  show-probabilities: off
  # Cache the function by default when running the prompt function
  cache: on
  vars:
  - "former"
  - "latter"
  examples:
  - "boysenberries"
  - "strawberries"
  # Completion is for generating a company-mode completion function
  # completion: on
  # # default values for pen -- evaled
  # # This is useful for completion commands.
  # pen-defaults:
  # - "(detect-language)"
  # - "(pen-preceding-text)"
  # These are elisp String->String functions and run from pen.el
  # It probably runs earlier than the preprocessors shell scripts
  pen-preprocessors:
  - "pen-pf-correct-grammar"
  # # A preprocessor filters the var at that position
  # the current implementation of preprocessors is kinda slow and will add ~100ml per variable
  # # This may be useful to distinguish a block of text, for example
  # preprocessors:
  # - "sed 's/^/- /"
  # - "cat"
  chomp-start: on
  chomp-end: off
  prefer-external: on
  # This is an optional external command which may be used to perform the same task as the API.
  # This can be used to train the prompt.
  # The external command must take arguments, not stdin
  # So that all variables may be passed into it.
  external: "generate-text-from-input.sh"
  # This compares the output of the external script to the output of the LM
  similarity-test:
  # This script returns a 0-1 decimal value representing the quality of the generated output.
  # The input is 2 arguments each containing output
  # The output is a decimal number from 0 to 1
  quality-script: "my-quality-checker-for-this-prompt.sh"
  # This script can be used to validate the output.
  # If the output is accurate, the validation script returns exit code 1.
  # The input is 2 arguments each containing output
  validation-script: "my-validator-for-this-prompt.sh"
  # Enable running conversation
  conversation-mode: yes
  # This is the name of an external database-driven pretext generator.
  # It would typically summarize and fact extract from history.
  # It then passes the pretext to the new prompt.
  conversation-pretext-generator: "code-interpreter"
  # Replace selected text
  filter: no
  # Keep stitching together until reaching this limit
  # This allows a full response for answers which may need n*max-tokens to reach the stop-sequence.
  stitch-max: 0
  needs-work: no
  n-test-runs: 5
  # Prompt function aliases
  # aliases:
  # - "asktutor"
  # postprocessor: "sed 's/- //' | uniqnosort"
  # # Run it n times and combine the output
  # n-collate: 10
  # This for combining prompts:
  # It might be, for example, summarize, or uniqnosort
  # collation-postprocessor: "uniqnosort"
#+END_SRC