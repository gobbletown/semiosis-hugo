#+LATEX_HEADER: \usepackage[margin=0.5in]{geometry}
#+OPTIONS: toc:nil

#+HUGO_BASE_DIR: /home/shane/var/smulliga/source/git/semiosis/semiosis-hugo
#+HUGO_SECTION: ./posts

#+TITLE: An LSP server for Codex and any language model
#+DATE: <2021-10-17>
#+AUTHOR: Shane Mulligan
#+KEYWORDS: openai codex pen emacs

** Summary
I set up an LSP server for OpenAI's
GPT-3/Codex or any LM via your favourite NLP
service provider.

First LSP server for LMs in the world, as far
as I know.

It gives documentation and refactoring to any
language including world languages and
fictional ones. As an LSP server, it works for
VSCode too.

The LSP server can create hover documentation,
code actions and linting services for any
programming language, world language, fictional
language or computing context.

** How does it work?
=Pen.el= utilises the =efm-langserver= along with its shell-interop.

Essentially, you design documentation and refactoring functions (prompt-functions) like so.

https://semiosis.github.io/posts/pen-el-host-interop-and-client-server/

*** efm-langserver
Then you configure EFM langserver like so:

https://github.com/mattn/efm-langserver

*** EFM Config
 #+BEGIN_SRC yaml -n :async :results verbatim code
     glossary1: &glossary1
       hover-command: '"penf" "pf-define-word-for-glossary/1"'
       hover-stdin: true
 #+END_SRC

*** emacs lisp
The LSP client communicates with the LSP server.

+ http://github.com/semiosis/pen.el/blob/master/src/pen-lsp-client.el

** Demo
#+BEGIN_EXPORT html
<!-- Play on asciinema.com -->
<!-- <a title="asciinema recording" href="https://asciinema.org/a/qCTVSRGZgUZruwuiW1JVaNI6t" target="_blank"><img alt="asciinema recording" src="https://asciinema.org/a/qCTVSRGZgUZruwuiW1JVaNI6t.svg" /></a> -->
<!-- Play on the blog -->
<script src="https://asciinema.org/a/qCTVSRGZgUZruwuiW1JVaNI6t.js" id="asciicast-qCTVSRGZgUZruwuiW1JVaNI6t" async></script>
#+END_EXPORT

** Final product
EFM Langserver is also built into the Pen.el docker image.